
<details open>
<summary><strong>2025 Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=social&label=Star)](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br>[When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios](https://arxiv.org/abs/2507.20198)<br>Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2507.20198)<br> [GitHub](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ZLKong/Awesome-Collection-Token-Reduction.svg?style=social&label=Star)](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br>[Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)<br>Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.18227)<br> [GitHub](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/yangdongchao/ALMTokenizer.svg?style=social&label=Star)](https://github.com/yangdongchao/ALMTokenizer)<br>[ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling](https://arxiv.org/abs/2504.10344)<br>Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen Meng |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.10344)<br> [GitHub](https://github.com/yangdongchao/ALMTokenizer)<br> | 
|  [![Publish](https://img.shields.io/badge/ECAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/andylee-24/token-pruning-audio-transformer.svg?style=social&label=Star)](https://github.com/andylee-24/token-pruning-audio-transformer)<br>[Token Pruning in Audio-Transformers: Optimizing Performance and Decoding Patch Importance](https://arxiv.org/abs/2504.01690)<br>Taehan Lee, Hyukjun Lee |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.01690)<br> [GitHub](https://github.com/andylee-24/token-pruning-audio-transformer)<br> [Model](https://drive.google.com/drive/folders/1cBDXh98m2qDlYLLX3q6xB-gtU1uUtxhK)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2.5-Omni)<br>[Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)<br>Qwen Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.20215)<br> [GitHub](https://github.com/QwenLM/Qwen2.5-Omni)<br> [Model](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/JeongHun0716/MMS-LLaMA.svg?style=social&label=Star)](https://github.com/JeongHun0716/MMS-LLaMA)<br>[MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens](https://arxiv.org/abs/2503.11315)<br>Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11315)<br> [GitHub](https://github.com/JeongHun0716/MMS-LLaMA)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs](https://arxiv.org/abs/2503.06362)<br>Umberto Cappellazzo, Minsu Kim, Stavros Petridis |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.06362)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Audio.svg?style=social&label=Star)](https://github.com/baichuan-inc/Baichuan-Audio)<br>[Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction](https://arxiv.org/abs/2502.17239)<br>Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17239)<br> [GitHub](https://github.com/baichuan-inc/Baichuan-Audio)<br> [Model](https://huggingface.co/baichuan-inc/Baichuan-Audio-Instruct)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.01-red)]() [![Star](https://img.shields.io/github/stars/VITA-MLLM/LUCY.svg?style=social&label=Star)](https://github.com/VITA-MLLM/LUCY)<br>[LUCY: Linguistic Understanding and Control Yielding Early Stage of Her](https://arxiv.org/abs/2501.16327)<br>Heting Gao, Hang Shao, Xiong Wang, Chaofan Qiu, Yunhang Shen, Siqi Cai, Yuchen Shi, Zihan Xu, Zuwei Long, Yike Zhang, Shaoqi Dong, Chaoyou Fu, Ke Li, Long Ma, Xing Sun |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2501.16327)<br> [GitHub](https://github.com/VITA-MLLM/LUCY)<br> [Model](https://huggingface.co/VITA-MLLM)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.01-red)]() [![Star](https://img.shields.io/github/stars/ASLP-lab/OSUM.svg?style=social&label=Star)](https://github.com/ASLP-lab/OSUM)<br>[OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia](https://arxiv.org/abs/2501.13306)<br>ASLP@NPU |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2501.13306)<br> [GitHub](https://github.com/ASLP-lab/OSUM)<br> [Model](https://huggingface.co/ASLP-lab/OSUM)<br> | 
</details>

<details open>
<summary><strong>2024 Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.12-red)]() [![Star](https://img.shields.io/github/stars/scb-10x/typhoon2-audio.svg?style=social&label=Star)](https://github.com/scb-10x/typhoon2-audio)<br>[Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models](https://arxiv.org/abs/2412.13702)<br>Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.13702)<br> [GitHub](https://github.com/scb-10x/typhoon2-audio)<br> [Model](https://huggingface.co/scb10x/llama3.1-typhoon2-audio-8b-instruct)<br> | 
|  [![Publish](https://img.shields.io/badge/ICME-2025-blue)]() <br>[SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval](https://arxiv.org/abs/2412.12009)<br>Yueqian Lin, Yuzhe Fu, Jingyang Zhang, Yudong Liu, Jianyi Zhang, Jingwei Sun, Hai "Helen" Li, Yiran Chen |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.12009)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star)](https://github.com/dvlab-research/Lyra)<br>[Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501)<br>Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.09501)<br> [GitHub](https://github.com/dvlab-research/Lyra)<br> [Model](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc)<br> [Dataset](https://huggingface.co/collections/zszhong/lyra-data-675d80fbab80334eb52cdd82)<br> | 
|  [![Publish](https://img.shields.io/badge/ICASSP-2025-blue)]() [![Star](https://img.shields.io/github/stars/umbertocappellazzo/Llama-AVSR.svg?style=social&label=Star)](https://github.com/umbertocappellazzo/Llama-AVSR)<br>[Large Language Models are Strong Audio-Visual Speech Recognition Learners](https://arxiv.org/abs/2409.12319)<br>Umberto Cappellazzo, Minsu Kim, Honglie Chen, Pingchuan Ma, Stavros Petridis, Daniele Falavigna, Alessio Brutti, Maja Pantic |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2409.12319)<br> [GitHub](https://github.com/umbertocappellazzo/Llama-AVSR)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/ictnlp/LLaMA-Omni.svg?style=social&label=Star)](https://github.com/ictnlp/LLaMA-Omni)<br>[LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://arxiv.org/abs/2409.06666)<br>Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2409.06666)<br> [GitHub](https://github.com/ictnlp/LLaMA-Omni)<br> [Model](https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni)<br> [Dataset](https://huggingface.co/datasets/ICTNLP/InstructS2S-200K)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.07-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-Audio.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2-Audio)<br>[Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759)<br>Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2407.10759)<br> [GitHub](https://github.com/QwenLM/Qwen2-Audio)<br> [Model](https://huggingface.co/collections/Qwen/qwen2-audio-66b628d694096020e0c52ff6)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2024-blue)]() [![Star](https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&label=Star)](https://github.com/bytedance/SALMONN/tree/videosalmonn)<br>[video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2406.15704)<br>Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, Chao Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2406.15704)<br> [GitHub](https://github.com/bytedance/SALMONN/tree/videosalmonn)<br> [Model](https://huggingface.co/tsinghua-ee/Video-SALMONN/tree/main)<br> | 
|  [![Publish](https://img.shields.io/badge/Interspeech-2024-blue)]() <br>[Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding](https://arxiv.org/abs/2406.13275)<br>Jizhong Liu, Gang Li, Junbo Zhang, Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Yujun Wang, Bin Wang |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2406.13275)<br> | 
|  [![Publish](https://img.shields.io/badge/Interspeech-2024-blue)]() [![Star](https://img.shields.io/github/stars/swarupbehera/FastAST.svg?style=social&label=Star)](https://github.com/swarupbehera/FastAST)<br>[FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation](https://arxiv.org/abs/2406.07676)<br>Swarup Ranjan Behera, Abhishek Dhiman, Karthik Gowda, Aalekhya Satya Narayani |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2406.07676)<br> [GitHub](https://github.com/swarupbehera/FastAST)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.06-red)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/VideoLLaMA2)<br>[VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/abs/2406.07476)<br>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2406.07476)<br> [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA2)<br> [Model](https://huggingface.co/collections/DAMO-NLP-SG/videollama2-6669b6b6f0493188305c87ed)<br> | 
|  [![Publish](https://img.shields.io/badge/Interspeech-2024-blue)]() <br>[Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model](https://arxiv.org/abs/2406.03706)<br>Jinlong Xue, Yayue Deng, Yicheng Han, Yingming Gao, Ya Li |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2406.03706)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.05-red)]() <br>[SpeechVerse: A Large-scale Generalizable Audio Language Model](https://arxiv.org/abs/2405.08295)<br>AWS AI Team |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2405.08295)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.02-red)]() <br>[An Embarrassingly Simple Approach for LLM with Strong ASR Capacity](https://arxiv.org/abs/2402.08846)<br>Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2402.08846)<br> | 
</details>

<details open>
<summary><strong>2023 Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICLR-2024-blue)]() [![Star](https://img.shields.io/github/stars/Render-AI/salmonn.svg?style=social&label=Star)](https://github.com/Render-AI/salmonn)<br>[SALMONN: Towards Generic Hearing Abilities for Large Language Models](https://arxiv.org/abs/2310.13289)<br>Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2310.13289)<br> [GitHub](https://github.com/Render-AI/salmonn)<br> [Model](https://huggingface.co/tsinghua-ee/SALMONN)<br> | 
|  [![Publish](https://img.shields.io/badge/ICASSP-2024-blue)]() <br>[Connecting Speech Encoder and Large Language Model for ASR](https://arxiv.org/abs/2309.13963)<br>Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2309.13963)<br> | 
|  [![Publish](https://img.shields.io/badge/ICASSP-2024-blue)]() <br>[Prompting Large Language Models with Speech Recognition Abilities](https://arxiv.org/abs/2307.11795)<br>Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2307.11795)<br> | 
|  [![Publish](https://img.shields.io/badge/Interspeech-2023-blue)]() <br>[Accelerating Transducers through Adjacent Token Merging](https://arxiv.org/abs/2306.16009)<br>Yuang Li, Yu Wu, Jinyu Li, Shujie Liu |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2306.16009)<br> | 
|  [![Publish](https://img.shields.io/badge/EMNLP-2023-blue)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/Video-LLaMA)<br>[Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)<br>Hang Zhang, Xin Li, Lidong Bing |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2306.02858)<br> [GitHub](https://github.com/DAMO-NLP-SG/Video-LLaMA)<br> [Model](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series)<br> | 
</details>

<details open>
<summary><strong>2022 Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICASSP-2022-blue)]() [![Star](https://img.shields.io/github/stars/RetroCirce/HTS-Audio-Transformer.svg?style=social&label=Star)](https://github.com/RetroCirce/HTS-Audio-Transformer)<br>[HTS-AT: A Hierarchical Token-Semantic Audio-Transformer for Sound Classification and Detection](https://arxiv.org/abs/2202.00874)<br>Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, Shlomo Dubnov |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2202.00874)<br> [GitHub](https://github.com/RetroCirce/HTS-Audio-Transformer)<br> | 
</details>
