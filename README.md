<div align=center>

# Awesome Multimodal Token Compression

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-2507\.20198-red.svg)](https://arxiv.org/abs/2507.20198)
[![Last Commit](https://img.shields.io/github/last-commit/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=flat&color=orange)](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)

[[arXiv]](https://arxiv.org/abs/2507.20198) [[HuggingFace]](https://huggingface.co/papers/2507.20198) [[Database]](https://oasis-paddleboat-fc1.notion.site/when-tokens-talk-too-much-database)

</div>

> **A Survey of Token Compression for Efficient Multimodal Large Language Models** [[arXiv]](https://arxiv.org/pdf/2507.20198)   
> [Kele Shao](https://cokeshao.github.io/)<sup>\*,1,2</sup>, [Keda Tao](https://kd-tao.github.io/)<sup>\*,1,2</sup>, [Kejia Zhang](https://kejiazhang-robust.github.io/)<sup>3</sup>, [Sicheng Feng](https://fscdc.github.io/)<sup>2,4</sup>, [Mu Cai](https://pages.cs.wisc.edu/~mucai/)<sup>5</sup>, [Yuzhang Shang](https://42shawn.github.io/)<sup>6</sup>, [Haoxuan You](https://hxyou.github.io/)<sup>7</sup>, [Can Qin](https://canqin.tech/)<sup>8</sup>, [Yang Sui](https://eclipsess.github.io/yangsui.github.io/)<sup>9</sup>, [Huan Wang](https://huanwang.tech/)<sup>‚Ä†,2</sup>
> 
> <sup>1</sup>Zhejiang University, <sup>2</sup>Westlake University, <sup>3</sup>Xiamen University, <sup>4</sup>National University of Singapore, <sup>5</sup>University of Wisconsin-Madison, <sup>6</sup>University of Central Florida, <sup>7</sup>Columbia University, <sup>8</sup>Salesforce AI Research, <sup>9</sup>Rice University
> 
> \* Equal Contribution.  ‚Ä† Corresponding Author (wanghuan@westlake.edu.cn).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/pulls) or [contact us](#Ô∏è-contact) to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2026.02.22]** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è We are very fortunate that our article was reported by [Êú∫Âô®‰πãÊòü](https://mp.weixin.qq.com/s/3Y4QNFV2wvsp_pv-habitg)!
- **[2026.02.22]** Paper accepted by **CVPR 2026** could be checked in [here](#published-in-recent-conferencejournal), welcome contributions!
- **[2026.01.27]** Paper accepted by **EMNLP 2025** and **ICLR 2026** could be checked in [here](#published-in-recent-conferencejournal).
- **[2026.01.24]** Our survey paper has been accepted to **TMLR 2026**. Congratulations! üéâüéâüéâ
- **[2025.10.11]** Papers accepted by **NeurIPS 2025** about MLLM token compression have been updated [here](#published-in-recent-conferencejournal). Congratulations! üéâüéâüéâ
- **[2025.08.14]** ‚ùó Added [Recent Papers](#recent-papers-last-6-months), [Papers Published in Recent Conference/Journal](#published-in-recent-conferencejournal), and a [database](https://oasis-paddleboat-fc1.notion.site/when-tokens-talk-too-much-database) for quick-search.
- **[2025.07.29]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation
<div align="left">
  <img src="images/motivation.png" alt="Awesome Token Compression" width="400"/>
</div>

> **Motivation:** **Up:** Image, video, and audio data types can scale in their representation dimensions, leading to a corresponding increase in the number of tokens. **Down:** Top-performing MLLMs cannot address real-world demands, as the number of tokens for multimodal information, especially video, vastly exceeds that of text. Therefore, token compression is crucial to address this limitation.

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{
shao2026a,
title={A Survey of Token Compression for Efficient Multimodal Large Language Models},
author={Kele Shao and Keda TAO and Kejia Zhang and Sicheng Feng and Mu Cai and Yuzhang Shang and Haoxuan You and Can Qin and Yang Sui and Huan Wang},
journal={Transactions on Machine Learning Research},
year={2026},
}
```

## üìö Contents

- [Awesome Token Compression](#awesome-multimodal-token-compression)
    - [Image LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/image-llm.md)
    - [Video LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/video-llm.md)
    - [Audio LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/audio-llm.md)
    - [Vision Transformer](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/vision-transformer.md)
    - [Audio Transformer](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/audio-transformer.md)

**Please check out all the papers by selecting the sub-area you're interested in. On this main page, only papers released in the past 6 months are shown.**

---

### Badge Colors
- ![arXiv Badge](https://img.shields.io/badge/arXiv-red) `red` for arXiv papers
- ![PDF Badge](https://img.shields.io/badge/PDF-blue) `blue` for conference/journal papers
- ![GitHub Badge](https://img.shields.io/badge/GitHub-white) `white` for GitHub repositories
- ![Research Areas Badge](https://img.shields.io/badge/Areas-purple) `purple` for research areas
- ![Categories Badge](https://img.shields.io/badge/Categories-green) `green` for categories
- ![Cost Badge](https://img.shields.io/badge/Cost-yellow) `yellow` for training cost

### Recent Papers (Last 6 Months)


<details open>
<summary><strong>Image</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() [![Star](https://img.shields.io/github/stars/hanxunyu/VisionTrim.svg?style=social&label=Star)](https://github.com/hanxunyu/VisionTrim)<br>[VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)<br>Hanxun Yu, Wentong Li, Xuan Qu, Song Wang, Junbo Chen, Jianke Zhu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2601.22674)<br> [GitHub](https://github.com/hanxunyu/VisionTrim)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/showlab.github.io/FocusUI.svg?style=social&label=Star)](https://showlab.github.io/FocusUI)<br>[FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection](https://arxiv.org/abs/2601.03928)<br>Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/GUI--Agent-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2601.03928)<br> [GitHub](https://showlab.github.io/FocusUI)<br> [Model](https://huggingface.co/yyyang/FocusUI-3B)<br> [Dataset](https://huggingface.co/datasets/yyyang/FocusUI-Training-Data)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[Prune Redundancy, Preserve Essence: Vision Token Compression in VLMs via Synergistic Importance-Diversity](https://arxiv.org/abs/2512.99999)<br> |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2512.99999)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() [![Star](https://img.shields.io/github/stars/MouxiaoHuang/PPE.svg?style=social&label=Star)](https://github.com/MouxiaoHuang/PPE)<br>[PPE: Positional Preservation Embedding for Token Compression in Multimodal Large Language Models](https://arxiv.org/abs/2510.22936)<br>Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.22936)<br> [GitHub](https://github.com/MouxiaoHuang/PPE)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.10-red)]() [![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR.svg?style=social&label=Star)](https://github.com/deepseek-ai/DeepSeek-OCR)<br>[DeepSeek-OCR: Contexts Optical Compression](https://arxiv.org/abs/2510.18234)<br>Haoran Wei, Yaofeng Sun, Yukun Li |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.18234)<br> [GitHub](https://github.com/deepseek-ai/DeepSeek-OCR)<br> [Model](https://huggingface.co/deepseek-ai/DeepSeek-OCR)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.10-red)]() [![Star](https://img.shields.io/github/stars/JulietChoo/VisionSelector.svg?style=social&label=Star)](https://github.com/JulietChoo/VisionSelector)<br>[VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)<br>Jiaying Zhu, Yurui Zhu, Xin Lu, Wenrui Yan, Dong Li, Kunlin Liu, Xueyang Fu, Zheng-Jun Zha |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.16598)<br> [GitHub](https://github.com/JulietChoo/VisionSelector)<br> [Model](https://huggingface.co/JulietChoo/VisionSelector-Qwen2.5-VL-7B)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.10-red)]() [![Star](https://img.shields.io/github/stars/Chenfei-Liao/VTC-Bench.svg?style=social&label=Star)](https://github.com/Chenfei-Liao/VTC-Bench)<br>[Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/abs/2510.07143)<br>Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2510.07143)<br> [GitHub](https://github.com/Chenfei-Liao/VTC-Bench)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models](https://arxiv.org/abs/2509.24837)<br>Youngeun Kim, Youjia Zhang, Huiling Liu, Aecheon Jung, Sunwoo Lee, Sungeun Hong |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.24837)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/AutoLab-SAI-SJTU/AutoPrune.svg?style=social&label=Star)](https://github.com/AutoLab-SAI-SJTU/AutoPrune)<br>[AutoPrune: Each Complexity Deserves a Pruning Policy](https://arxiv.org/abs/2509.23931)<br>Hanshi Wang, Yuhao Xu, Zekun Xu, Jin Gao, Yufan Liu, Weiming Hu, Ke Wang, Zhipeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.23931)<br> [GitHub](https://github.com/AutoLab-SAI-SJTU/AutoPrune)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score](https://arxiv.org/abs/2509.23663)<br>Jingqi Xu, Jingxi Lu, Chenghao Li, Sreetama Sarkar, Peter A. Beerel |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.23663)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance](https://arxiv.org/abs/2509.15704)<br>Yuxuan Liang, Xu Li, Xiaolei Chen, Yi Zheng, Haotian Chen, Bin Li, Xiangyang Xue |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.15704)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)<br>Jingyu Xiao, Zhongyi Zhang, Yuxuan Wan, Yintong Huo, Yang Liu, Michael R.Lyu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/GUI--Agent-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.12159)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)<br>Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis, Sami Muhaidat |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.09955)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star)](https://github.com/OpenGVLab/InternVL)<br>[InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265)<br>InternVL Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.18265)<br> [GitHub](https://github.com/OpenGVLab/InternVL)<br> [Model](https://huggingface.co/collections/OpenGVLab/internvl35-68ac87bd52ebe953485927fb)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)<br>Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.17857)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[Revisiting MLLM Token Technology through the Lens of Classical Visual Coding](https://arxiv.org/abs/2508.13460)<br>Jinming Liu, Junyan Lin, Yuntao Wei, Kele Shao, Keda Tao, Jianguo Huang, Xudong Yang, Zhibo Chen, Huan Wang, Xin Jin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Visual--Coding-purple)]() |  |  [Paper](https://arxiv.org/abs/2508.13460)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)<br>Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Shao Tang, Sayan Ghosh, Xuanzhao Dong, Rajat Koner, Yalin Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.11886)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)<br>Yanshu Li, Jianjiang Yang, Zhennan Shen, Ligong Han, Haoyan Xu, Ruixiang Tang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.07871)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)<br>Weichen Zhang, Zhui Zhu, Ningbo Li, Kebin Liu, Yunhao Liu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.06084)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)<br>Huanyu Wang, Jushi Kai, Haoli Bai, Lu Hou, Bo Jiang, Ziwei He, Zhouhan Lin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.06038)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() [![Star](https://img.shields.io/github/stars/HVision-NKU/GlimpsePrune.svg?style=social&label=Star)](https://github.com/HVision-NKU/GlimpsePrune)<br>[A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.01548)<br>Quan-Sheng Zeng, Yunheng Li, Qilong Wang, Peng-Tao Jiang, Zuxuan Wu, Ming-Ming Cheng, Qibin Hou |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01548)<br> [GitHub](https://github.com/HVision-NKU/GlimpsePrune)<br> [Model](https://huggingface.co/collections/ashun989/glimpseprune-688d8826ef5bd09db6af145e)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)<br>Mingyu Fu, Wei Suo, Ji Ma, Lin Yuanbo Wu, Peng Wang, Yanning Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01236)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)<br>Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, Bin Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.00553)<br> | 
</details>

<details open>
<summary><strong>Video</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.12-red)]() <br>[EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)<br>Chao Gong, Depeng Wang, Zhipeng Wei, Ya Guo, Huijia Zhu, Jingjing Chen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2512.10324)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/lern-to-write/STC.svg?style=social&label=Star)](https://github.com/lern-to-write/STC)<br>[Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)<br>Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Streaming--Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2512.00891)<br> [GitHub](https://github.com/lern-to-write/STC)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)<br>Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2511.00141)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() [![Star](https://img.shields.io/github/stars/MouxiaoHuang/PPE.svg?style=social&label=Star)](https://github.com/MouxiaoHuang/PPE)<br>[PPE: Positional Preservation Embedding for Token Compression in Multimodal Large Language Models](https://arxiv.org/abs/2510.22936)<br>Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.22936)<br> [GitHub](https://github.com/MouxiaoHuang/PPE)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/YIGE24/StreamingTOM.svg?style=social&label=Star)](https://github.com/YIGE24/StreamingTOM)<br>[StreamingTOM: Streaming Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.18269)<br>Xueyi Chen, Keda Tao, Kele Shao, Huan Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Streaming--Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2510.18269)<br> [GitHub](https://github.com/YIGE24/StreamingTOM)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() <br>[Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)<br>Vaggelis Dorovatas, Soroush Seifi, Gunshi Gupta, Rahaf Aljundi |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2510.17364)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.10-red)]() [![Star](https://img.shields.io/github/stars/JulietChoo/VisionSelector.svg?style=social&label=Star)](https://github.com/JulietChoo/VisionSelector)<br>[VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)<br>Jiaying Zhu, Yurui Zhu, Xin Lu, Wenrui Yan, Dong Li, Kunlin Liu, Xueyang Fu, Zheng-Jun Zha |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.16598)<br> [GitHub](https://github.com/JulietChoo/VisionSelector)<br> [Model](https://huggingface.co/JulietChoo/VisionSelector-Qwen2.5-VL-7B)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)<br>Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.07915)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning](https://arxiv.org/abs/2509.22481)<br>Xiangmo Zhao, Nan Yang, Yang Wang, Zhanwen Liu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Event--Camera-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.22481)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)<br>Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/VLN-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.15250)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() <br>[The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning](https://arxiv.org/abs/2509.12594)<br>Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/VLA-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2509.12594)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.09-red)]() [![Star](https://img.shields.io/github/stars/Zizzzzzzz/FocusMamba.svg?style=social&label=Star)](https://github.com/Zizzzzzzz/FocusMamba)<br>[Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)<br>Nan Yang, Yang Wang, Zhanwen Liu, Yuchao Dai, Yang Liu, Xiangmo Zhao |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Event--Camera-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2509.03872)<br> [GitHub](https://github.com/Zizzzzzzz/FocusMamba)<br> | 
|  [![Publish](https://img.shields.io/badge/EMNLP_Oral-2025-blue)]() [![Star](https://img.shields.io/github/stars/NIneeeeeem/LangDC.svg?style=social&label=Star)](https://github.com/NIneeeeeem/LangDC)<br>[Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors](https://arxiv.org/abs/2509.00969)<br>Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2509.00969)<br> [GitHub](https://github.com/NIneeeeeem/LangDC)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star)](https://github.com/OpenGVLab/InternVL)<br>[InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265)<br>InternVL Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.18265)<br> [GitHub](https://github.com/OpenGVLab/InternVL)<br> [Model](https://huggingface.co/collections/OpenGVLab/internvl35-68ac87bd52ebe953485927fb)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)<br>Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.17857)<br> | 
|  [![Publish](https://img.shields.io/badge/EMNLP--blue)]() [![Star](https://img.shields.io/github/stars/yogesh-iitj/LGTTP.svg?style=social&label=Star)](https://github.com/yogesh-iitj/LGTTP)<br>[Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing](https://arxiv.org/abs/2508.17686)<br>Yogesh Kumar |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.17686)<br> [GitHub](https://github.com/yogesh-iitj/LGTTP)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() [![Star](https://img.shields.io/github/stars/zju-jiyicheng/SpecVLM.svg?style=social&label=Star)](https://github.com/zju-jiyicheng/SpecVLM)<br>[SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)<br>Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() |  [Paper](https://arxiv.org/abs/2508.16201)<br> [GitHub](https://github.com/zju-jiyicheng/SpecVLM)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding](https://arxiv.org/abs/2508.15717)<br>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Streaming--Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.15717)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)<br>Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Shao Tang, Sayan Ghosh, Xuanzhao Dong, Rajat Koner, Yalin Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.11886)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
</details>

<details open>
<summary><strong>Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.12-red)]() <br>[EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)<br>Chao Gong, Depeng Wang, Zhipeng Wei, Ya Guo, Huijia Zhu, Jingjing Chen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2512.10324)<br> | 
</details>

<details open>
<summary><strong>Omni</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/KD-TAO/OmniZip.svg?style=social&label=Star)](https://github.com/KD-TAO/OmniZip)<br>[OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models](https://arxiv.org/abs/2511.14582)<br>Keda Tao, Kele Shao, Bohan Yu, Weiqiang Wang, Jian liu, Huan Wang |  [![Area](https://img.shields.io/badge/Omni--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2511.14582)<br> [GitHub](https://github.com/KD-TAO/OmniZip)<br> | 
</details>


### Published in Recent Conference/Journal


<details open>
<summary><strong>CVPR 2026</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/showlab.github.io/FocusUI.svg?style=social&label=Star)](https://showlab.github.io/FocusUI)<br>[FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection](https://arxiv.org/abs/2601.03928)<br>Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/GUI--Agent-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2601.03928)<br> [GitHub](https://showlab.github.io/FocusUI)<br> [Model](https://huggingface.co/yyyang/FocusUI-3B)<br> [Dataset](https://huggingface.co/datasets/yyyang/FocusUI-Training-Data)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/lern-to-write/STC.svg?style=social&label=Star)](https://github.com/lern-to-write/STC)<br>[Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)<br>Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Streaming--Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2512.00891)<br> [GitHub](https://github.com/lern-to-write/STC)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/KD-TAO/OmniZip.svg?style=social&label=Star)](https://github.com/KD-TAO/OmniZip)<br>[OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models](https://arxiv.org/abs/2511.14582)<br>Keda Tao, Kele Shao, Bohan Yu, Weiqiang Wang, Jian liu, Huan Wang |  [![Area](https://img.shields.io/badge/Omni--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2511.14582)<br> [GitHub](https://github.com/KD-TAO/OmniZip)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2026-blue)]() [![Star](https://img.shields.io/github/stars/YIGE24/StreamingTOM.svg?style=social&label=Star)](https://github.com/YIGE24/StreamingTOM)<br>[StreamingTOM: Streaming Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.18269)<br>Xueyi Chen, Keda Tao, Kele Shao, Huan Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Streaming--Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2510.18269)<br> [GitHub](https://github.com/YIGE24/StreamingTOM)<br> | 
</details>

<details open>
<summary><strong>ICLR 2026</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() [![Star](https://img.shields.io/github/stars/hanxunyu/VisionTrim.svg?style=social&label=Star)](https://github.com/hanxunyu/VisionTrim)<br>[VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)<br>Hanxun Yu, Wentong Li, Xuan Qu, Song Wang, Junbo Chen, Jianke Zhu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2601.22674)<br> [GitHub](https://github.com/hanxunyu/VisionTrim)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[Prune Redundancy, Preserve Essence: Vision Token Compression in VLMs via Synergistic Importance-Diversity](https://arxiv.org/abs/2512.99999)<br> |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2512.99999)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)<br>Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2511.00141)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() [![Star](https://img.shields.io/github/stars/MouxiaoHuang/PPE.svg?style=social&label=Star)](https://github.com/MouxiaoHuang/PPE)<br>[PPE: Positional Preservation Embedding for Token Compression in Multimodal Large Language Models](https://arxiv.org/abs/2510.22936)<br>Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.22936)<br> [GitHub](https://github.com/MouxiaoHuang/PPE)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)<br>Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2510.07915)<br> | 
|  [![Publish](https://img.shields.io/badge/ICLR-2026-blue)]() <br>[Task-Related Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/abs/2506.01097)<br> |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  |  [Paper](https://arxiv.org/abs/2506.01097)<br> | 
</details>

<details open>
<summary><strong>EMNLP 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/EMNLP_Oral-2025-blue)]() [![Star](https://img.shields.io/github/stars/NIneeeeeem/LangDC.svg?style=social&label=Star)](https://github.com/NIneeeeeem/LangDC)<br>[Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors](https://arxiv.org/abs/2509.00969)<br>Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2509.00969)<br> [GitHub](https://github.com/NIneeeeeem/LangDC)<br> | 
</details>

<details open>
<summary><strong>NeurIPS 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() <br>[Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)<br>Vaggelis Dorovatas, Soroush Seifi, Gunshi Gupta, Rahaf Aljundi |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2510.17364)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/AutoLab-SAI-SJTU/AutoPrune.svg?style=social&label=Star)](https://github.com/AutoLab-SAI-SJTU/AutoPrune)<br>[AutoPrune: Each Complexity Deserves a Pruning Policy](https://arxiv.org/abs/2509.23931)<br>Hanshi Wang, Yuhao Xu, Zekun Xu, Jin Gao, Yufan Liu, Weiming Hu, Ke Wang, Zhipeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2509.23931)<br> [GitHub](https://github.com/AutoLab-SAI-SJTU/AutoPrune)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/VisionThink.svg?style=social&label=Star)](https://github.com/dvlab-research/VisionThink)<br>[VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)<br>Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.13348)<br> [GitHub](https://github.com/dvlab-research/VisionThink)<br> [Model](https://huggingface.co/collections/Senqiao/visionthink-6878d839fae02a079c9c7bfe)<br> [Dataset](https://huggingface.co/collections/Senqiao/visionthink-6878d839fae02a079c9c7bfe)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/Theia-4869/CDPruner.svg?style=social&label=Star)](https://github.com/Theia-4869/CDPruner)<br>[Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/abs/2506.10967)<br>Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.10967)<br> [GitHub](https://github.com/Theia-4869/CDPruner)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/yunzhuzhang0918/flexselect.svg?style=social&label=Star)](https://github.com/yunzhuzhang0918/flexselect)<br>[FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)<br>Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, Linchao Zhu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.00993)<br> [GitHub](https://github.com/yunzhuzhang0918/flexselect)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() <br>[Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)<br>Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.22038)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/cokeshao/HoliTom.svg?style=social&label=Star)](https://github.com/cokeshao/HoliTom)<br>[HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)<br>Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.21334)<br> [GitHub](https://github.com/cokeshao/HoliTom)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() <br>[Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)<br>Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.10118)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hai-chao-Zhang/VQToken.svg?style=social&label=Star)](https://github.com/Hai-chao-Zhang/VQToken)<br>[VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models](https://arxiv.org/abs/2503.16980)<br>Haichao Zhang, Yun Fu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.16980)<br> [GitHub](https://github.com/Hai-chao-Zhang/VQToken)<br> [Model](https://huggingface.co/haichaozhang/VQ-Token-llava-ov-0.5b)<br> | 
|  [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/LunarShen/FastVID.svg?style=social&label=Star)](https://github.com/LunarShen/FastVID)<br>[FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187)<br>Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11187)<br> [GitHub](https://github.com/LunarShen/FastVID)<br> | 
</details>

<details open>
<summary><strong>ICCV 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/mlvlab/Representation-Shift.svg?style=social&label=Star)](https://github.com/mlvlab/Representation-Shift)<br>[Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)<br>Joonmyung Choi, Sanghyeok Lee, Byungoh Ko, Eunseo Kim, Jihyung Kil, Hyunwoo J. Kim |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.00367)<br> [GitHub](https://github.com/mlvlab/Representation-Shift)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/YuchenLiu98/METEOR.svg?style=social&label=Star)](https://github.com/YuchenLiu98/METEOR)<br>[METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/abs/2507.20842)<br>Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.20842)<br> [GitHub](https://github.com/YuchenLiu98/METEOR)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)](https://github.com/HYUNJS/STTM)<br>[Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video-LLMs](https://arxiv.org/abs/2507.07990)<br>Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.07990)<br> [GitHub](https://github.com/HYUNJS/STTM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)<br>Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.02591)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/zwl666666/Skip-Vision.svg?style=social&label=Star)](https://github.com/zwl666666/Skip-Vision)<br>[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](https://arxiv.org/abs/2503.21817)<br>Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.21817)<br> [GitHub](https://github.com/zwl666666/Skip-Vision)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[Growing a Twig to Accelerate Large Vision-Language Models](https://arxiv.org/abs/2503.14075)<br>Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.14075)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LSDBench.svg?style=social&label=Star)](https://github.com/dvlab-research/LSDBench)<br>[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](https://arxiv.org/abs/2503.12496)<br>Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2503.12496)<br> [GitHub](https://github.com/dvlab-research/LSDBench)<br> [Dataset](https://huggingface.co/datasets/TainU/LSDBench)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/anakin-skywalker-Joseph/Folder.svg?style=social&label=Star)](https://github.com/anakin-skywalker-Joseph/Folder)<br>[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/abs/2501.02430)<br>Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, Victor Qu√©tu, Shuai Xiao, Enzo Tartaglione |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2501.02430)<br> [GitHub](https://github.com/anakin-skywalker-Joseph/Folder)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/thu-nics/FrameFusion.svg?style=social&label=Star)](https://github.com/thu-nics/FrameFusion)<br>[FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models](https://arxiv.org/abs/2501.01986)<br>Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, Yu Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() |  [Paper](https://arxiv.org/abs/2501.01986)<br> [GitHub](https://github.com/thu-nics/FrameFusion)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM.svg?style=social&label=Star)](https://github.com/Hon-Wong/ByteVideoLLM)<br>[Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/abs/2412.09530)<br>Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li, Haiyang Yu, Jinghui Lu, Can Huang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.09530)<br> [GitHub](https://github.com/Hon-Wong/ByteVideoLLM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star)](https://github.com/dvlab-research/Lyra)<br>[Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501)<br>Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.09501)<br> [GitHub](https://github.com/dvlab-research/Lyra)<br> [Model](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc)<br> [Dataset](https://huggingface.co/collections/zszhong/lyra-data-675d80fbab80334eb52cdd82)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/LaVi-Lab/AIM.svg?style=social&label=Star)](https://github.com/LaVi-Lab/AIM)<br>[AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)<br>Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.03248)<br> [GitHub](https://github.com/LaVi-Lab/AIM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Theia-4869/VisPruner.svg?style=social&label=Star)](https://github.com/Theia-4869/VisPruner)<br>[Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs](https://arxiv.org/abs/2412.01818)<br>Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.01818)<br> [GitHub](https://github.com/Theia-4869/VisPruner)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/abs/2410.08584)<br>Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2410.08584)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/joslefaure/HERMES.svg?style=social&label=Star)](https://github.com/joslefaure/HERMES)<br>[HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics](https://arxiv.org/abs/2408.17443)<br>Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, Winston H. Hsu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2408.17443)<br> [GitHub](https://github.com/joslefaure/HERMES)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/42Shawn/LLaVA-PruMerge.svg?style=social&label=Star)](https://github.com/42Shawn/LLaVA-PruMerge)<br>[LLaVA-PruMerge:¬†Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388)<br>Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2403.15388)<br> [GitHub](https://github.com/42Shawn/LLaVA-PruMerge)<br> | 
</details>

<details open>
<summary><strong>ACL 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/EffiVLM-Bench/EffiVLM-Bench.svg?style=social&label=Star)](https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br>[EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models](https://arxiv.org/abs/2506.00479)<br>Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, Bing Qin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2506.00479)<br> [GitHub](https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/JeongHun0716/MMS-LLaMA.svg?style=social&label=Star)](https://github.com/JeongHun0716/MMS-LLaMA)<br>[MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens](https://arxiv.org/abs/2503.11315)<br>Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11315)<br> [GitHub](https://github.com/JeongHun0716/MMS-LLaMA)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/AIoT-MLSys-Lab/MEDA.svg?style=social&label=Star)](https://github.com/AIoT-MLSys-Lab/MEDA)<br>[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)<br>Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17599)<br> [GitHub](https://github.com/AIoT-MLSys-Lab/MEDA)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/Visual-AI/PruneVid.svg?style=social&label=Star)](https://github.com/Visual-AI/PruneVid)<br>[PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/abs/2412.16117)<br>Xiaohu Huang, Hao Zhou, Kai Han |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.16117)<br> [GitHub](https://github.com/Visual-AI/PruneVid)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL_Oral-2025-blue)]() [![Star](https://img.shields.io/github/stars/ZongqianLi/Prompt-Compression-Survey.svg?style=social&label=Star)](https://github.com/ZongqianLi/Prompt-Compression-Survey)<br>[Prompt Compression for Large Language Models: A Survey](https://arxiv.org/abs/2410.12388)<br>Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier |  [![Area](https://img.shields.io/badge/LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2410.12388)<br> [GitHub](https://github.com/ZongqianLi/Prompt-Compression-Survey)<br> | 
</details>

<details open>
<summary><strong>ICML 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/wangqinsi1/2025-ICML-CoreMatching.svg?style=social&label=Star)](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br>[CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)<br>Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.19235)<br> [GitHub](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/yangdongchao/ALMTokenizer.svg?style=social&label=Star)](https://github.com/yangdongchao/ALMTokenizer)<br>[ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling](https://arxiv.org/abs/2504.10344)<br>Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen Meng |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.10344)<br> [GitHub](https://github.com/yangdongchao/ALMTokenizer)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/steven-ccq/ViLAMP.svg?style=social&label=Star)](https://github.com/steven-ccq/ViLAMP)<br>[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)<br>Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.02438)<br> [GitHub](https://github.com/steven-ccq/ViLAMP)<br> [Model](https://huggingface.co/orange-sk/ViLAMP-llava-qwen)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star)](https://github.com/Vision-CAIR/LongVU)<br>[LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/abs/2410.17434)<br>Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2410.17434)<br> [GitHub](https://github.com/Vision-CAIR/LongVU)<br> [Model](https://huggingface.co/collections/Vision-CAIR/longvu-67181d2debabfc1eb050c21d)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Gumpest/SparseVLMs.svg?style=social&label=Star)](https://github.com/Gumpest/SparseVLMs)<br>[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)<br>Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2410.04417)<br> [GitHub](https://github.com/Gumpest/SparseVLMs)<br> | 
</details>

<details open>
<summary><strong>ACM MM 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)<br>Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.17857)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)<br>Mingyu Fu, Wei Suo, Ji Ma, Lin Yuanbo Wu, Peng Wang, Yanning Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01236)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)](https://github.com/yaolinli/TimeChat-Online)<br>[TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)<br>Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.17343)<br> [GitHub](https://github.com/yaolinli/TimeChat-Online)<br> [Model](https://huggingface.co/wyccccc/TimeChatOnline-7B)<br> [Dataset](https://huggingface.co/datasets/yaolily/TimeChat-Online-139K)<br> | 
</details>


---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

This repository is inspired by [Awesome-Efficient-Reasoning-Models](https://github.com/fscdc/Awesome-Efficient-Reasoning-Models), [Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM/), [Awesome-Context-Engineering](https://github.com/Meirtz/Awesome-Context-Engineering)

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [shaokele@gmail.com](mailto:shaokele@gmail.com) / [KD.TAO.CT@outlook.com](mailto:KD.TAO.CT@outlook.com)

## ‚ú® Star History

[![Star History Chart](https://api.star-history.com/svg?repos=cokeshao/Awesome-Multimodal-Token-Compression&type=date&legend=top-left)](https://www.star-history.com/#cokeshao/Awesome-Multimodal-Token-Compression&type=date&legend=top-left)

[**‚¨Ü Back to top**](#awesome-multimodal-token-compression)
