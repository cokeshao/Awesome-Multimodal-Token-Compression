
## 2025

*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501)  [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Qwen2.5-Omni Technical Report](https://arxiv.org/pdf/2503.20215)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens](https://arxiv.org/abs/2503.11315v1)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs](https://arxiv.org/abs/2503.06362)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction](https://arxiv.org/abs/2502.17239)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.01-red)]() [OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia](https://arxiv.org/abs/2501.13306)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()

## 2024

*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.12-red)]() [Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models](https://arxiv.org/abs/2412.13702)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICME-2025-blue)]() [SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval](https://arxiv.org/abs/2412.12009)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.09-red)]() [Large Language Models are Strong Audio-Visual Speech Recognition Learners](https://arxiv.org/abs/2409.12319)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.09-red)]() [LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://arxiv.org/abs/2409.06666)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.07-red)]() [Qwen2-Audio Technical Report](https://arxiv.org/pdf/2407.10759)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.07-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-Audio.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2-Audio) [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICML-2024-blue)]() [video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2406.15704)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/Interspeech-2024-blue)]() [Enhancing Automated Audio Captioning via Large Language Models with
Optimized Audio Encoding](https://arxiv.org/abs/2406.13275)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.06-red)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/VideoLLaMA2) [VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/abs/2406.07476)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/Interspeech-2024-blue)]() [Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model](https://arxiv.org/abs/2406.03706)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.05-red)]() [SpeechVerse: A Large-scale Generalizable Audio Language Model](https://arxiv.org/abs/2405.08295)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.05-red)]() [SpeechVerse: A Large-scale Generalizable Audio Language Model](https://arxiv.org/abs/2405.08295)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.02-red)]() [An Embarrassingly Simple Approach for LLM with Strong ASR Capacity](https://arxiv.org/abs/2402.08846)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()

## 2023

*  [![Publish](https://img.shields.io/badge/ICLR-2024-blue)]() [SALMONN: Towards Generic Hearing Abilities for Large Language Models](https://arxiv.org/abs/2310.13289)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2023\.09-red)]() [Connecting Speech Encoder and Large Language Model for ASR](https://arxiv.org/abs/2309.13963)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICASSP-2024-blue)]() [Prompting Large Language Models with Speech Recognition Abilities](https://arxiv.org/pdf/2307.11795)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2023\.06-red)]() [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
